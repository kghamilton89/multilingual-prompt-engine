{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import markdown\n",
    "import os\n",
    "import requests\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/kenneth.hamilton/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['huggingface-cli', 'login', '--token', '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'], returncode=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'your_huggingface_token' with your actual Hugging Face token\n",
    "huggingface_token = \"PLEASE ADD YOUR HF TOKEN\"\n",
    "\n",
    "# Log in to Hugging Face\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", huggingface_token])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Prompt 1:\n",
      "Учитывая текст: Опытный и энтузиастичный новатор... вы хотите в своей команде. Маргарет Хайнс является основателем и главным консультантом Inspire Marketing, LLC, инвестирующей в местные предприятия, обслуживающей сообщество бизнес-брокером и маркетинговым консультированием. Она имеет степень бакалавра в Вашингтонском университете в Сент-Луисе, штат Москва, и MBA из Университета Висконсинка-Милваки. Маргарет предлагает консультации в области маркетинга, продаж бизнеса и поворотов и франчайзинга.\n",
      "\n",
      "Translated Prompt 2:\n",
      "В настоящее время компания находится в неактивном состоянии. Главный адрес компании VOSS GROUP, LLC: 6537 FAIRWAY HILL CT, ОРЛАНДО, FL, 32835. Между тем, вы можете отправить свои письма на 6537 FAIRWAY HILL CT, ОРЛАНДО, FL, 32835.\n",
      "\n",
      "Translated Prompt 3:\n",
      "Используя описательный язык, напишите абзац, который полностью уловит запах свежецветкой розы, включая нюансы и тонкости аромата, а также любые эмоции или воспоминания, которые она может вызвать. Используйте сенсорные детали, такие как текстура, температура и цвет, чтобы оживить этот аромат для читателя. Рассмотрите, какие другие запахи или запахи могут быть обнаружены в окружающей области, и как они могут взаимодействовать с аромами розы.\n",
      "\n",
      "Translated Prompt 4:\n",
      "Как любитель ИИ, вы любите создавать программы, которые могут понимать человеческий язык. Ваш последний проект включает в себя создание программы, которая может идентифицировать и заменить слова их антонимиками в данном тексте. Чтобы продемонстрировать эффективность вашей программы, вы решаете проверить ее в новостной статье о недавнем политическом событии. Однако, чтобы сделать это более сложным, вы также хотите, чтобы ваша программа различала между гомонимами и использовала контекстные подсказки, чтобы правильно их заменить. Вот пошаговое объяснение того, как работает ваша программа: 1. Программа читает входный текст и идентифицирует все слова, которые имеют антонимики. 2. Для каждого из этих слов программа идентифицирует контекст, в котором она кажется определить значение оригинального антонимика. 3. Программа затем\n",
      "\n",
      "Translated Prompt 5:\n",
      "какую роль играют транзистор пропуска и операционный ампер в регуляторе\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset('DIBT/MPEP_RUSSIAN')\n",
    "\n",
    "# Extract all \"target-suggestion\" entries\n",
    "target_suggestions = [item.get('target-suggestion', 'No target-suggestion found') for item in dataset['train']]\n",
    "\n",
    "# Display the first 5 prompts to translate to confirm successful upload\n",
    "for i, suggestion in enumerate(target_suggestions[:5], 1):\n",
    "    print(f\"Translated Prompt {i}:\\n{suggestion}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hf_inference(model, prompt, token):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.post(api_url, headers=headers, json={\"inputs\": prompt})\n",
    "    \n",
    "    # Check for valid response\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            result = response.json()\n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                return result[0].get('generated_text', 'No generated text found')\n",
    "            else:\n",
    "                return \"Unexpected response format\"\n",
    "        except ValueError:\n",
    "            return \"Invalid JSON response\"\n",
    "    else:\n",
    "        return f\"Error {response.status_code}: {response.text}\"\n",
    "\n",
    "# List of models to use\n",
    "models = [\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"Qwen/Qwen2-72B-Instruct\",\n",
    "    # Add other models here\n",
    "]\n",
    "\n",
    "# Number of prompts to process\n",
    "num_prompts = 10\n",
    "\n",
    "# Capture responses\n",
    "all_responses = []\n",
    "for model in models:\n",
    "    model_responses = []\n",
    "    for prompt in target_suggestions[:num_prompts]:\n",
    "        response_text = query_hf_inference(model, prompt, huggingface_token)\n",
    "        model_responses.append({\"model\": model, \"prompt\": prompt, \"response\": response_text})\n",
    "    all_responses.append((model, model_responses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_responses_to_markdown(all_responses, filename=\"responses.md\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for model, responses in all_responses:\n",
    "            f.write(f\"# Model: {model}\\n\\n\")\n",
    "            for i, response in enumerate(responses, 1):\n",
    "                f.write(f\"## Prompt {i}\\n\")\n",
    "                f.write(f\"{response['prompt']}\\n\\n\")\n",
    "                f.write(f\"## Response {i}\\n\")\n",
    "                f.write(f\"{response['response']}\\n\\n\")\n",
    "\n",
    "# Write responses to markdown file\n",
    "write_responses_to_markdown(all_responses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
